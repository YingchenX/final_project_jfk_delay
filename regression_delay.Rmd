---
title: "Regression_delay"
output: github_document
---


```{r setup, include=FALSE}
#Setup

library(tidyverse)
library(viridis)
library(modelr)
library(mgcv)
library(dplyr)
library(tidyr)
library(skimr)
library(plotly)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
set.seed(1)
```


```{r data wrangling, include=FALSE}
#Dataset
delay = read.csv("./tidied_data/delay.csv") %>% 
  janitor::clean_names() %>% filter(delay_minutes > 0)

h_weather = read.csv("./tidied_data/hourly_weather.csv") %>% 
  janitor::clean_names() 

#Check how many airlines
unique(delay$airline_name) #_7_ -> ok, keep

#Check how many destinations
unique(delay$destination_airport) #_66_ -> too many, remove

#Keep variables of interest and `date` for merge purpose (which will be removed later) 
## Clean dataset 'delay'
delay = delay %>% 
  rename(
    airline = airline_name,
    hour = scheduled_hour,
    delay = delay_minutes,
    carrierd = delay_carrier_minutes,
    extrmwd = delay_weather_minutes,
    nasd = delay_national_aviation_system_minutes,
    securityd = delay_security_minutes,
    latarrd = delay_late_aircraft_arrival_minutes) %>% 
  mutate(hour = as.numeric(hour),
         month = as.factor(month.abb[month]),
         airline = as.factor(airline)) %>% 
  select(airline, date, month, hour, delay, carrierd, extrmwd, nasd, securityd, latarrd)
  
#check 'NA'
sum(is.na(delay)) #_0_ -> good

## Clean dataset 'h_weather'
### About the measure of temperature: Since the dry bulb temperature is the ambient air temperature measured by regular thermometers, that is, the temperature often mentioned in our general weather forecast. Thus, we decide to use the variable `hourly_dry_bulb_temperature` to represent temperature.
h_weather = h_weather %>% 
  rename(
    temperature = hourly_dry_bulb_temperature,
    humidity = hourly_relative_humidity,
    visibility = hourly_visibility,
    wind_s = hourly_wind_speed) %>% 
  mutate(hour = as.numeric(hour),
         month = as.factor(month.abb[month])) %>% 
  select(date, month, hour, temperature, humidity, visibility, wind_s)

#check 'NA'
sum(is.na(h_weather)) #_0_ -> good

## Merge datasets 'delay' and 'hourly_weather'

raw_df = merge(x = delay, y = h_weather, by = c("date", "month", "hour"),
               all.x = TRUE) %>% 
  mutate(hour_c = cut(hour, breaks = c(4, 8, 13, 17, 24),
                      labels = c("morning","noon","afternoon","night"))) %>% 
  mutate(hour_c = as.factor(hour_c)) %>% 
  select(-hour)

#check 'NA'
sum(is.na(raw_df)) #_0_ -> good

###To this step, we have our raw dataset for doing association analysis. Since the outcome variable `delay` is a continuous variable, we would do linear regression. However, there might be too many variables so far (ignoring `date` we still got 12 potential predictors), so next step will be fitting the model.

### By intuition, I would set: `airline` -> categorical `month` -> categorical `hour` -> categorical (need further categorization); and for the rest -> continuous

## Inspections into the dataset

# Let's first check how each variable is roughly distributed.

### Dependent variable / outcome (continuous)
###See if our dependent variable `delay` follows a normal distribution.

#his_plot = raw_df %>% 
  #plot_ly(x = ~delay, 
   #type = "histogram", marker = list(color = 'rgb(255, 156, 128)'))

#his_plot %>% 
         #layout(
         #title = 'Distribution of hourly delay in time, 2021/11/1 to 2022/1/31',
         #xaxis = list(title = "Hourly delay in minutes", tickangle = -45),
         #yaxis = list(title = "Frequency"))

### No, our dependent variable `delay` is not normally distributed, but we dont want to remove those outliers since they are important to our analysis. -> It is okay because for linear regression, dependent variable does not have to be normally distributed. On the other hand, the model's residuals, do have to be normally distributed. 

# Given the above histogram, we dicide to break the outcome into 2 parts: (1) bearable delay and (2) extreme delay

delay_bear = delay %>% filter(delay <= 100)
hist(delay_bear$delay)

delay_ext = delay %>% filter(delay > 100)
hist(delay_ext$delay)
```

# Overview

...

# Data description

...

# Step 1: Data Exploration

put it here OR put in the exploration page

## Check distribution of the outcome (continuous)

if not related to modeling, delete, or include it in the exploration page.

```{r}
his = raw_df %>% 
  plot_ly(x = ~delay, 
   type = "histogram", marker = list(color = 'rgb(255, 156, 128)'))

his %>% 
         layout(
         title = 'Distribution of hourly delay in time, 2021/11/1 to 2022/1/31',
         xaxis = list(title = "Hourly delay in minutes", tickangle = -45),
         yaxis = list(title = "Frequency"))
```

## Inspect into the effect of 3 hypothesized categorical predictors

_Motivation:_

To get a sense of the effect of each predictor.

_Plan:_

We first produce boxplots to visualize the distribution of delay time across each group. Then, we perform ANOVA to see if each group have the same average delay time. For those significant, we perform pairwise t-tests using Bonferroniâ€™s correction for the p-values to calculate pairwise differences between the delay time of each group.

```{r function for boxplot, include=FALSE}
box_cat = function(cat){
  
  box = raw_df %>% 
  plot_ly(x = ~cat, y = ~delay, color = ~cat,
          type = 'box', colors = "viridis") %>%  
  layout(
    yaxis = list(title = "Delay Time (minutes)"))

}
```

### Airlines

```{r}
#boxplot
box_cat(raw_df$airline) %>% 
  layout(xaxis = list(title = "Airline"))

#One-way ANOVA
aov(delay ~ airline, data = raw_df) %>% summary()

#Pairwise t-tests (Bonferroni)
pairwise.t.test(raw_df$delay, raw_df$airline, p.adjust.method = "bonferroni")
```

_Highlights:_

* Since the overall p-value (<2e-16) is less than .05, this is an indication that each airline group does not have the same average delay time (in minutes).

* The adjusted p-value for the mean difference in delay time between American Airlines _vs._ JetBlue Airways is 3.6e-06.

* The adjusted p-value for the mean difference in delay time between Delta Air Lines _vs._ JetBlue Airways < 2e-16.


### Months

```{r}
#boxplot
box_cat(raw_df$month) %>% 
  layout(xaxis = list(title = "Month"))

#One-way ANOVA
aov(delay ~ month, data = raw_df) %>% summary()

#Pairwise t-tests (Bonferroni)
pairwise.t.test(raw_df$delay, raw_df$month, p.adjust.method = "bonferroni")
```

_Highlights:_

* Since the overall p-value (<2e-16) is less than .05, this is an indication that each month group does not have the same average delay time (in minutes).

* The adjusted p-value for the mean difference in delay time between November, 2021 _vs._ December, 2021 is .0015.

* The adjusted p-value for the mean difference in delay time between November, 2021 _vs._ January, 2022 and December, 2021 _vs._ January, 2022 both < 2e-16.


### Times of the day ("hour_c")

```{r}
#boxplot
box_cat(raw_df$hour_c) %>% 
  layout(xaxis = list(title = "Times of the day"))

#One-way ANOVA
aov(delay ~ hour_c, data = raw_df) %>% summary()

#Pairwise t-tests (Bonferroni)
pairwise.t.test(raw_df$delay, raw_df$hour_c, p.adjust.method = "bonferroni")
```

_Highlights:_

* Since the overall p-value (0.000146) is less than .05, this is an indication that each hour group does not have the same average delay time (in minutes).

* The adjusted p-value for the mean difference in delay time between morning _vs._ night is 6.7e-05.

* The adjusted p-value for the mean difference in delay time between noon _vs._ afternoon is 1.00.


### Independent variables / predictors (categorical)

cat_sum = raw_df %>% 
  select(airline, month, hour) %>% 
  mutate(
    airline = as.factor(airline),
    month = as.factor(month),
    hour = as.factor(hour)
    ) %>% 
  summary(maxsum = 24)

cat_sum

### Independent variables / predictors (continuous)

con_sum_df = raw_df %>% 
  select(-date, -airline, -month, -hour)

con_sum = skim(con_sum_df) %>%
  dplyr::select(-n_missing, -complete_rate) %>% 
  mutate(
    mean = numeric.mean,
    sd = numeric.sd,
    histogram = numeric.hist,
    var = skim_variable,
    min = numeric.p0,
    max = numeric.p100,
    median = numeric.p50,
    q1 = numeric.p25,
    q3 = numeric.p75
  ) %>% 
  dplyr::select(-numeric.mean, -numeric.sd, -numeric.hist, -skim_variable, -numeric.p0, -numeric.p100, -numeric.p50, -numeric.p25, -numeric.p75) %>% 
  relocate(var, mean, min, q1, median, q3, max, sd, histogram)

con_sum


Next, as mentioned above, we want to further categorize variable `hour`.

_Motivation:_ 

To increase the power of our model by reducing the number of parameters involved and to be more efficient and concise.

_Rationale for the categorization of `hour`:_

Based on the previous inspection of variable `hour`, we could see that: except for 5, 8, 10, 22, and 23, the frequencies of the other classes are roughly even (between 1000-2000). Take this into consideration, our rationale for classification will be a combination of convention and the desire to achieve a uniform distribution.

Thus, we would categorize `hour` into the following 4 categories:

_morning_: 5-8

_noon_: 9-13

_afternoon_: 14-17 

_night_:18-23 

Now, we can start categorize `hour`


As usual, check if it was done properly
```{r}
summary(as.factor(raw_df$hour_c))
sum(is.na(raw_df))
```


_0_ 'NA' and the distribution looks good.

Now, since we are not yet able to decide the variable type of the rest of the predictors, we need further analysis.

## Step 2: Check Assumptions for Regression

## Assumption 1: Linearity

Do simple linear regression for each independent variable, along with scatterplots to assess the linearity

### Continuous vars

* temperature

```{r message=FALSE, warning=FALSE}
lrTemp = lm(delay~temperature, data = raw_df)
summary(lrTemp) %>% broom::glance()
summary(lrTemp) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~temperature, data = raw_df)
```

* humidity

```{r message=FALSE, warning=FALSE}
lrHum = lm(delay~humidity, data = raw_df)
summary(lrHum) %>% broom::glance()
summary(lrHum) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~humidity, data = raw_df)
```

* visibility

```{r message=FALSE, warning=FALSE}
lrVis = lm(delay~visibility, data = raw_df)
summary(lrVis) %>% broom::glance()
summary(lrVis) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~visibility, data = raw_df)
```

* wind speed

```{r message=FALSE, warning=FALSE}
lrWin = lm(delay~wind_s, data = raw_df)
summary(lrWin) %>% broom::glance()
summary(lrWin) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~wind_s, data = raw_df)
```

* carrier delay

```{r message=FALSE, warning=FALSE}
lrCar = lm(delay~carrierd, data = raw_df)
summary(lrCar) %>% broom::glance()
summary(lrCar) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~carrierd, data = raw_df)
```

* extreme weather delay

```{r message=FALSE, warning=FALSE}
lrExw = lm(delay~extrmwd, data = raw_df)
summary(lrExw) %>% broom::glance()
summary(lrExw) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~extrmwd, data = raw_df)
```

* NAS delay

```{r message=FALSE, warning=FALSE}
lrNas = lm(delay~nasd, data = raw_df)
summary(lrNas) %>% broom::glance()
summary(lrNas) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~nasd, data = raw_df)
```

* security delay

```{r message=FALSE, warning=FALSE}
lrSec = lm(delay~securityd, data = raw_df)
summary(lrSec) %>% broom::glance()
summary(lrSec) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) 

plot(delay~securityd, data = raw_df)
```

* late arrival delay

```{r message=FALSE, warning=FALSE}
lrLat = lm(delay~latarrd, data = raw_df)
summary(lrLat) %>% broom::glance()
summary(lrLat) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)

plot(delay~latarrd, data = raw_df)
```

### Categorical vars

First, tell R that they are categorical variables
```{r}
raw_df = 
  raw_df %>% 
  mutate(
    month = fct_infreq(as.factor(month)),
    hour_c = fct_infreq(hour_c),
    airline = fct_infreq(airline)
    )
```

* month

```{r message=FALSE, warning=FALSE}
lrMon = lm(delay~month, data = raw_df)
summary(lrMon) %>% broom::glance()
summary(lrMon) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)
```

* hour

```{r}
lrHour = lm(delay~hour_c, data = raw_df)
summary(lrHour) %>% broom::glance()
summary(lrHour) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)
```

* airline

```{r}
lrAL = lm(delay~airline, data = raw_df)
summary(lrAL) %>% broom::glance()
summary(lrAL) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)
```

The above results were not very straightforward, but give us a sense that we could include `carrierd` and `latarrd` in to the final model, according to the R-squared values.


## Assumption 2: Independence of observations

Use correlation to assess the relationship between all independent variables and make sure they arenâ€™t too highly correlated.

First, it's time to remove variable `date` as it is not one of the predictors

```{r}
raw_df = raw_df %>% 
  select(-date)
```

Correlation matrix 

_NOTE:_ This approach is not meaningful for our nominal predictors `month`, `airline`, or `hour_c`. you can ignore them.

```{r}
cor = raw_df %>% 
  select(-delay) %>% 
  mutate(
    airline = as.numeric(airline),
    month = as.numeric(month),
    hour_c = as.numeric(hour_c)
    ) %>% 
  cor(method = c("pearson", "kendall", "spearman"))

round(cor, 2)

color = colorRampPalette(c("Blue", "white", "Red"))(20)
heatmap(x = cor, col = color, symm = TRUE)
```

Looks good, pretty independent.

*Could considering removing 1 of (`visibility` and `humidity`) as they have a moderate correlation coefficient  (-0.54)


## Assumption 3: Normality

Check this after we make the model.

## Assumption 4: Homoscedasticity

Check this after we make the model.

# Step 3: Building linear regression model

## Perform linear regression with all predictors
```{r}
lm_all = lm(delay ~ ., data = raw_df)
summary(lm_all)
```

_Note:_ 

R-squared = 0.9467 -> good 

F-statistic: 
F-value = 2.639e+04 -> large, variance between >> variance within, good  
p-value < 2.2e-16 -> small, significant, good

_The F-test of overall significance indicates whether this current linear regression model provides a better fit to the data than a model that contains no independent variables._

`humidity` -> p-value = 0.497590 (not significant) under this model
-> given what's mentioned above, remove 

`wind_s` -> p-value = 0.618192 (not significant) under this model
-> remove


Fit lm without  `wind_s` or `humidity`
```{r}
raw_df_10 = raw_df %>% 
  select(-humidity, -wind_s)

lm_10 = lm(delay ~ ., data = raw_df_10)
summary(lm_10)
```

_Note:_ 

R-squared = 0.9467 -> same, good 

F-statistic: 
F-value = 2.932e+04 -> close, large, variance between >> variance within, good
p-value < 2.2e-16 -> same, small, significant, good

## Fit the best subset linear model

The function regsubsets() will produce the best model with 1 predictor, the best model with 2 predictors, 3 predictors, â€¦ up to 14 predictors(nvmax=14 option).

```{r}
library(leaps)

bestsub.lm <- regsubsets(delay ~ ., 
                            data = raw_df_10, nvmax = 18)

sum.bestsub.lm = summary(bestsub.lm)
```

We have 18 predictor parameters, and the sample size 29725 will be sufficient for it ("One in ten" rule)

Check some measures to select the best subset model

_Note:_

A small value of Cp means that the model is relatively precise.

A larger R-squared value means that the independent variables explain a larger percentage of the variation in the independent variable.

A lower BIC implies either fewer explanatory variables, better fit, or both.

```{r}
cbind( 
    Cp = summary(bestsub.lm)$cp,
    r2 = summary(bestsub.lm)$rsq,
    BIC = summary(bestsub.lm)$bic
)
```


```{r}
which.max(sum.bestsub.lm$rsq)
```


```{r}
sum.bestsub.lm$which[18,]
```

All true -> keep all

## Fit the regression model
```{r}
Best_lm = lm(delay ~  .,
              data = raw_df_10)
summary(Best_lm) %>% broom::glance()
summary(Best_lm) %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value)
  
```

## Check for residual normality
```{r}
library("olsrr")

ols_plot_resid_fit(Best_lm)

# residual vs fitted
plot(Best_lm, 1)

#qq plot
plot(Best_lm, 2)


```

## Check for heteroscadacity


```{r}
plot(Best_lm, 3)
```


# Step 4: Fit stratum-specific models

Stratum of interest: 

1. month

2. airline

3. hour_c

## Fit month-specific model

we can nest within months and fit month-specific models associating delay with the rest of variables

```{r}
nest_lm_m =
  raw_df_10 %>% 
  select(-airline, -hour_c) %>% 
  nest(data = -month) %>% 
  mutate(
    models = map(data, ~lm(delay ~ ., data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)

nest_lm_m %>% 
  select(month, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, values_from = estimate) %>% 
  knitr::kable(digits = 3)
```


## Fit airline-specific model

we can nest within airlines and fit airline-specific models associating delay with the rest of variables

```{r}
nest_lm_a =
  raw_df_10 %>% 
  select(-month, -hour_c) %>% 
  nest(data = -airline) %>% 
  mutate(
    models = map(data, ~lm(delay ~ ., data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)

nest_lm_a %>% 
  select(airline, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, values_from = estimate) %>% 
  knitr::kable(digits = 3)
```


## Fit hour-specific model

we can nest within hour categories and fit hour-specific models associating delay with the rest of variables

```{r}
nest_lm_h =
  raw_df_10 %>% 
  select(-airline, -month) %>% 
  nest(data = -hour_c) %>% 
  mutate(
    models = map(data, ~lm(delay ~ ., data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)

nest_lm_h %>% 
  select(hour_c, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, values_from = estimate) %>% 
  knitr::kable(digits = 3)
```


# Step 5: Add interaction terms
## Carrier * airline 
```{r}
int1_lm = lm(delay ~ . + carrierd*airline,
              data = raw_df_10)

anova(int1_lm)
```

significant

